{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be54b56-923e-424b-9be1-247f0e118b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import explode, split, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ae414d-2207-494c-af8b-a42e40564ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store execution times\n",
    "# execution_times = {}\n",
    "\n",
    "def measure_execution_time(job_name, job_function, job_parameter=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the Spark job function\n",
    "    if job_parameter:\n",
    "        job_function(job_parameter)\n",
    "    else:\n",
    "        job_function()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    # Store the execution time using the job name as the key\n",
    "    # execution_times[job_name] = execution_time\n",
    "    print(f\"Execution time for {job_name}: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23f74b4-18d6-4743-be2c-25fbab43498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words or topics in the content and summaries of the top N subreddits?\n",
    "\n",
    "def job_common_words_top_subreddits(df):\n",
    "    # First, identify the top N subreddits by post count\n",
    "    top_subreddits = df.groupBy(\"subreddit\").count().orderBy(col(\"count\").desc()).limit(100)\n",
    "    \n",
    "    # Filter posts from these top subreddits\n",
    "    top_subreddit_posts = df.join(top_subreddits, \"subreddit\")\n",
    "    \n",
    "    # Split words in content and explode the resulting arrays to analyze word frequency\n",
    "    words_df = top_subreddit_posts.select(explode(split(col(\"content\"), \" \")).alias(\"word\"))\n",
    "    word_counts = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
    "    \n",
    "    word_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "560428cb-4163-467a-a833-a475e9d9bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_scaling_experiment(n_executor_instances: int):\n",
    "    spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.240:7077\") \\\n",
    "        .appName(\"scaling_experiments\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.mminExecutors\", n_executor_instances)\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", n_executor_instances)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    \n",
    "    spark_context.setLogLevel(\"WARN\")\n",
    "    df = spark_session.read.json(\"hdfs://192.168.2.240:9000/corpus-webis-tldr-17.json\")\n",
    "    measure_execution_time(\"Common Words in Top Subreddits\", job_common_words_top_subreddits, df)\n",
    "    spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae5798a-c457-4c7e-a9d6-314979c28505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 14:56:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|word|   count|\n",
      "+----+--------+\n",
      "|   I|21772577|\n",
      "| the|21371259|\n",
      "|  to|20370477|\n",
      "| and|19591881|\n",
      "|   a|15566019|\n",
      "|  of|11112888|\n",
      "|  \\n| 9323606|\n",
      "|that| 8404942|\n",
      "|  in| 7778927|\n",
      "|  my| 7162255|\n",
      "| was| 6725528|\n",
      "|  is| 6225561|\n",
      "| for| 5952877|\n",
      "|  it| 5789218|\n",
      "|    | 5659071|\n",
      "|with| 5334123|\n",
      "|  me| 4367853|\n",
      "| but| 4345200|\n",
      "|  on| 4274986|\n",
      "|have| 4207031|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 1060.2244713306427 seconds\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|word|   count|\n",
      "+----+--------+\n",
      "|   I|21772577|\n",
      "| the|21371259|\n",
      "|  to|20370477|\n",
      "| and|19591881|\n",
      "|   a|15566019|\n",
      "|  of|11112888|\n",
      "|  \\n| 9323606|\n",
      "|that| 8404942|\n",
      "|  in| 7778927|\n",
      "|  my| 7162255|\n",
      "| was| 6725528|\n",
      "|  is| 6225561|\n",
      "| for| 5952877|\n",
      "|  it| 5789218|\n",
      "|    | 5659071|\n",
      "|with| 5334123|\n",
      "|  me| 4367853|\n",
      "| but| 4345200|\n",
      "|  on| 4274986|\n",
      "|have| 4207031|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 425.0892810821533 seconds\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|word|   count|\n",
      "+----+--------+\n",
      "|   I|21772577|\n",
      "| the|21371259|\n",
      "|  to|20370477|\n",
      "| and|19591881|\n",
      "|   a|15566019|\n",
      "|  of|11112888|\n",
      "|  \\n| 9323606|\n",
      "|that| 8404942|\n",
      "|  in| 7778927|\n",
      "|  my| 7162255|\n",
      "| was| 6725528|\n",
      "|  is| 6225561|\n",
      "| for| 5952877|\n",
      "|  it| 5789218|\n",
      "|    | 5659071|\n",
      "|with| 5334123|\n",
      "|  me| 4367853|\n",
      "| but| 4345200|\n",
      "|  on| 4274986|\n",
      "|have| 4207031|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 399.14378809928894 seconds\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|word|   count|\n",
      "+----+--------+\n",
      "|   I|21772577|\n",
      "| the|21371259|\n",
      "|  to|20370477|\n",
      "| and|19591881|\n",
      "|   a|15566019|\n",
      "|  of|11112888|\n",
      "|  \\n| 9323606|\n",
      "|that| 8404942|\n",
      "|  in| 7778927|\n",
      "|  my| 7162255|\n",
      "| was| 6725528|\n",
      "|  is| 6225561|\n",
      "| for| 5952877|\n",
      "|  it| 5789218|\n",
      "|    | 5659071|\n",
      "|with| 5334123|\n",
      "|  me| 4367853|\n",
      "| but| 4345200|\n",
      "|  on| 4274986|\n",
      "|have| 4207031|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 244.2339735031128 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4]:\n",
    "    print(i)\n",
    "    strong_scaling_experiment(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c869fa7-8756-4402-9af3-80f815c92790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_scaling_experiment(n_executor_instances: int):\n",
    "    spark_session = SparkSession.builder\\\n",
    "        .master(\"spark://192.168.2.240:7077\") \\\n",
    "        .appName(\"scaling_experiments\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 4)\\\n",
    "        .config(\"spark.dynamicAllocation.mminExecutors\", n_executor_instances)\\\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", n_executor_instances)\\\n",
    "        .config(\"spark.driver.port\",9999)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "    # RDD API\n",
    "    spark_context = spark_session.sparkContext\n",
    "    \n",
    "    spark_context.setLogLevel(\"WARN\")\n",
    "    df = spark_session.read.json(\"hdfs://192.168.2.240:9000/corpus-webis-tldr-17.json\")\n",
    "    if n_executor_instances < 4:\n",
    "        test_partition_size = 0.25*n_executor_instances\n",
    "        partitions = df.randomSplit([test_partition_size, 1-test_partition_size], seed=42)  # Split into four equal parts\n",
    "        test_partition = partitions[0]\n",
    "    else:\n",
    "        test_partition = df\n",
    "    measure_execution_time(\"Common Words in Top Subreddits\", job_common_words_top_subreddits, test_partition)\n",
    "    spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfabec-6a17-4a56-a351-3f5748c6faee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "|   I|5445867|\n",
      "| the|5342233|\n",
      "|  to|5100828|\n",
      "| and|4900189|\n",
      "|   a|3893890|\n",
      "|  of|2780404|\n",
      "|  \\n|2330324|\n",
      "|that|2102802|\n",
      "|  in|1946803|\n",
      "|  my|1791453|\n",
      "| was|1681171|\n",
      "|  is|1558996|\n",
      "| for|1490589|\n",
      "|  it|1451590|\n",
      "|    |1438665|\n",
      "|with|1334852|\n",
      "|  me|1091820|\n",
      "| but|1086165|\n",
      "|  on|1070809|\n",
      "|have|1054204|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 602.5473868846893 seconds\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|word|   count|\n",
      "+----+--------+\n",
      "|   I|10884028|\n",
      "| the|10676129|\n",
      "|  to|10180626|\n",
      "| and| 9794095|\n",
      "|   a| 7783599|\n",
      "|  of| 5555264|\n",
      "|  \\n| 4657986|\n",
      "|that| 4199831|\n",
      "|  in| 3887963|\n",
      "|  my| 3581214|\n",
      "| was| 3358200|\n",
      "|  is| 3114524|\n",
      "| for| 2976653|\n",
      "|  it| 2897434|\n",
      "|    | 2847490|\n",
      "|with| 2665830|\n",
      "|  me| 2183117|\n",
      "| but| 2169807|\n",
      "|  on| 2138446|\n",
      "|have| 2106767|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution time for Common Words in Top Subreddits: 336.02891635894775 seconds\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                       (0 + 12) / 147]\r"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4]:\n",
    "    print(i)\n",
    "    weak_scaling_experiment(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
